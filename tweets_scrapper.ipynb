{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Scrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def web_scrapping(links_list,filesname_list):\n",
    "    try:\n",
    "        time.sleep(1)  # Allow 2 seconds for the web page to open\n",
    "        scroll_pause_time = 5  # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "        #driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "       # screen_height = driver.execute_script(\"return window.screen.height;\")  # get the screen height of the web\n",
    "        \n",
    "        check = 0\n",
    "        \n",
    "\n",
    "        tweets_list = []   # list of products\n",
    "        for j,link in enumerate(links_list):\n",
    "            driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "            screen_height = driver.execute_script(\"return window.screen.height;\")  # get the screen height of the web\n",
    "            #print(\"screen_height>>>>>>>>> \",screen_height)\n",
    "            driver.get(link)\n",
    "            file_name = filesname_list[j]\n",
    "            flag = False\n",
    "            print(\"Starting File >> \",filesname_list[j])\n",
    "            time.sleep(15)\n",
    "            i = 1\n",
    "            while True:\n",
    "\n",
    "                # scroll one screen height each time\n",
    "                driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))\n",
    "                i += 1\n",
    "                time.sleep(scroll_pause_time)\n",
    "                # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "                scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "                #print(\"scroll_height >>>> \",scroll_height)\n",
    "                # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "                for parent in soup.find_all(\"div\", class_=\"css-901oao r-18jsvk2 r-37j5jr r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-bnwqim r-qvutc0\"):\n",
    "                    print(i,\">>>\",parent.text)\n",
    "                    tweets_list.append(parent.text)\n",
    "\n",
    "                tweets_list = list(set(tweets_list))\n",
    "                check = check + 1\n",
    "                if flag == True:\n",
    "                    df1 =pd.read_csv(file_name)\n",
    "                    df1.drop_duplicates(inplace = True,keep=\"first\")\n",
    "                else:\n",
    "                    df1 = pd.DataFrame()\n",
    "                    flag  = True\n",
    "\n",
    "                df2 = pd.DataFrame({\"new_colum\":tweets_list})\n",
    "                df2.drop_duplicates(inplace = True)\n",
    "\n",
    "                frames = [df1, df2]\n",
    "\n",
    "                result = pd.concat(frames,axis=0)    \n",
    "                #print(\"\\n Before >>\",len(result))\n",
    "                result.dropna(how='all',inplace=True)\n",
    "                print(\"After >>\",len(result))\n",
    "                result.to_csv(file_name, index=False)\n",
    "                tweets_list = []\n",
    "\n",
    "                if (screen_height) * i > scroll_height:\n",
    "                    print(\"Ending File >> \",filesname_list[j])\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"[!] \",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the scrapper to gathe the data for cricket tweets\n",
    "links_list = [\"https://twitter.com/search?q=%23ipl&src=typed_query&f=top\",\"https://twitter.com/search?q=%23PSL2022&src=typeahead_click&f=top\",\"https://twitter.com/search?q=%23cricket&src=typed_query&f=top\",\"https://twitter.com/search?q=%23movies&src=typed_query&f=top\",\"https://twitter.com/search?q=%23netflix&src=typed_query&f=top\",\"https://twitter.com/search?q=%23hollywood%23movies&src=typed_query&f=top\"]\n",
    "files_name = [\"ipl_latest1.csv\",\"psl_latest1.csv\",\"cricket_latest1.csv\",\"movies_latest1.csv\",\"netflix_latest1.csv\",\"hollywoodMovies1.csv\"]\n",
    "files_name = files_name[::-1]\n",
    "links_list = links_list[::-1]\n",
    "web_scrapping(links_list,files_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
